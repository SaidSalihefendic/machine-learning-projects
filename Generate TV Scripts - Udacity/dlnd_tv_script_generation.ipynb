{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TV Script Generation\n",
    "\n",
    "In this project, you'll generate your own [Seinfeld](https://en.wikipedia.org/wiki/Seinfeld) TV scripts using RNNs.  You'll be using part of the [Seinfeld dataset](https://www.kaggle.com/thec03u5/seinfeld-chronicles#scripts.csv) of scripts from 9 seasons.  The Neural Network you'll build will generate a new ,\"fake\" TV script, based on patterns it recognizes in this training data.\n",
    "\n",
    "## Get the Data\n",
    "\n",
    "The data is already provided for you in `./data/Seinfeld_Scripts.txt` and you're encouraged to open that file and look at the text. \n",
    ">* As a first step, we'll load in this data and look at some samples. \n",
    "* Then, you'll be tasked with defining and training an RNN to generate a new script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# load in data\n",
    "import helper\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_line_range` to view different parts of the data. This will give you a sense of the data you'll be working with. You can see, for example, that it is all lowercase text, and each new line of dialogue is separated by a newline character `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 10000 to 10020:\n",
      "helen: we don't use it. \n",
      "\n",
      "morty: what are you talking? we use it. \n",
      "\n",
      "helen: if you were using it, we wouldn't use it. \n",
      "\n",
      "jerry: so what would you do? you'd hitch? \n",
      "\n",
      "helen: how much is a rent-a-car? \n",
      "\n",
      "jerry: i don't know. 25 bucks a day. \n",
      "\n",
      "helen: what? you're crazy. \n",
      "\n",
      "morty: plus the insurance. \n",
      "\n",
      "jerry: oh, i didn't get the insurance. \n",
      "\n",
      "morty: how could you not get the insurance? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (10000, 10020)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Implement Pre-processing Functions\n",
    "The first thing to do to any dataset is pre-processing.  Implement the following pre-processing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following **tuple** `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "        Create lookup tables for vocabulary\n",
    "        :param text: The text of tv scripts split into words\n",
    "        :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Now count the words and sort them to turn vocab_to_int and int_to_vocab\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word: ii for ii, word in enumerate(vocab)}\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(vocab)}\n",
    "\n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks can create multiple ids for the same word. For example, \"bye\" and \"bye!\" would generate two different word ids.\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( **.** )\n",
    "- Comma ( **,** )\n",
    "- Quotation Mark ( **\"** )\n",
    "- Semicolon ( **;** )\n",
    "- Exclamation mark ( **!** )\n",
    "- Question mark ( **?** )\n",
    "- Left Parentheses ( **(** )\n",
    "- Right Parentheses ( **)** )\n",
    "- Dash ( **-** )\n",
    "- Return ( **\\n** )\n",
    "\n",
    "This dictionary will be used to tokenize the symbols and add the delimiter (space) around it.  This separates each symbols as its own word, making it easier for the neural network to predict the next word. Make sure you don't use a value that could be confused as a word; for example, instead of using the value \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenized dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tokenized_punctuation = {\n",
    "        '.': '<PERIOD>',\n",
    "        ',': '<COMMA>',\n",
    "        '\"': '<QUOTATION_MARK>',\n",
    "        ';': '<SEMICOLON>',\n",
    "        '!': '<EXCLAMATION_MARK>',\n",
    "        '?': '<QUESTION_MARK>',\n",
    "        '(': '<LEFT_PARENTHESES>',\n",
    "        ')': '<RIGHT_PARENTHESES>',\n",
    "        '-': '<DASH>',\n",
    "        '\\n': '<RETURN>',\n",
    "    }\n",
    "        \n",
    "    return tokenized_punctuation\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process all the data and save it\n",
    "\n",
    "Running the code cell below will pre-process all the data and save it to file. You're encouraged to lok at the code for `preprocess_and_save_data` in the `helpers.py` file to see what it's doing in detail, but you do not need to change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# pre-process training data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "from workspace_utils import active_session\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "In this section, you'll build the components necessary to build an RNN by implementing the RNN Module and forward and backpropagation functions.\n",
    "\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input\n",
    "Let's start with the preprocessed input data. We'll use [TensorDataset](http://pytorch.org/docs/master/data.html#torch.utils.data.TensorDataset) to provide a known format to our dataset; in combination with [DataLoader](http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader), it will handle batching, shuffling, and other dataset iteration functions.\n",
    "\n",
    "You can create data with TensorDataset by passing in feature and target tensors. Then create a DataLoader as usual.\n",
    "```\n",
    "data = TensorDataset(feature_tensors, target_tensors)\n",
    "data_loader = torch.utils.data.DataLoader(data, \n",
    "                                          batch_size=batch_size)\n",
    "```\n",
    "\n",
    "### Batching\n",
    "Implement the `batch_data` function to batch `words` data into chunks of size `batch_size` using the `TensorDataset` and `DataLoader` classes.\n",
    "\n",
    ">You can batch words using the DataLoader, but it will be up to you to create `feature_tensors` and `target_tensors` of the correct size and content for a given `sequence_length`.\n",
    "\n",
    "For example, say we have these as input:\n",
    "```\n",
    "words = [1, 2, 3, 4, 5, 6, 7]\n",
    "sequence_length = 4\n",
    "```\n",
    "\n",
    "Your first `feature_tensor` should contain the values:\n",
    "```\n",
    "[1, 2, 3, 4]\n",
    "```\n",
    "And the corresponding `target_tensor` should just be the next \"word\"/tokenized word value:\n",
    "```\n",
    "5\n",
    "```\n",
    "This should continue with the second `feature_tensor`, `target_tensor` being:\n",
    "```\n",
    "[2, 3, 4, 5]  # features\n",
    "6             # target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    \"\"\"\n",
    "    Batch the neural network data using DataLoader\n",
    "    :param words: The word ids of the TV scripts\n",
    "    :param sequence_length: The sequence length of each batch\n",
    "    :param batch_size: The size of each batch; the number of sequences in a batch\n",
    "    :return: DataLoader with batched data\n",
    "    \"\"\"\n",
    "    # TODO: Implement function\n",
    "    \n",
    "    # We want to make a list of words with sequence lengths\n",
    "    features = np.zeros((len(words) - sequence_length, sequence_length), dtype=int)\n",
    "    targets = np.zeros((len(words) - sequence_length,), dtype=int)\n",
    "    \n",
    "    for i in range(len(words) - sequence_length):\n",
    "        features[i] = np.asarray(words[i: i + sequence_length], dtype=int)\n",
    "        targets[i] = np.asarray(words[i + sequence_length], dtype=int)\n",
    "    \n",
    "    data = TensorDataset(torch.from_numpy(features), torch.from_numpy(targets)) \n",
    "    loader = DataLoader(data, shuffle=True, batch_size=batch_size)\n",
    "    # return a dataloader\n",
    "    return loader\n",
    "\n",
    "# there is no test for this function, but you are encouraged to create\n",
    "# print statements and tests of your own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your dataloader \n",
    "\n",
    "You'll have to modify this code to test a batching function, but it should look fairly similar.\n",
    "\n",
    "Below, we're generating some test text data and defining a dataloader using the function you defined, above. Then, we are getting some sample batch of inputs `sample_x` and targets `sample_y` from our dataloader.\n",
    "\n",
    "Your code should return something like the following (likely in a different order, if you shuffled your data):\n",
    "\n",
    "```\n",
    "torch.Size([10, 5])\n",
    "tensor([[ 28,  29,  30,  31,  32],\n",
    "        [ 21,  22,  23,  24,  25],\n",
    "        [ 17,  18,  19,  20,  21],\n",
    "        [ 34,  35,  36,  37,  38],\n",
    "        [ 11,  12,  13,  14,  15],\n",
    "        [ 23,  24,  25,  26,  27],\n",
    "        [  6,   7,   8,   9,  10],\n",
    "        [ 38,  39,  40,  41,  42],\n",
    "        [ 25,  26,  27,  28,  29],\n",
    "        [  7,   8,   9,  10,  11]])\n",
    "\n",
    "torch.Size([10])\n",
    "tensor([ 33,  26,  22,  39,  16,  28,  11,  43,  30,  12])\n",
    "```\n",
    "\n",
    "### Sizes\n",
    "Your sample_x should be of size `(batch_size, sequence_length)` or (10, 5) in this case and sample_y should just have one dimension: batch_size (10). \n",
    "\n",
    "### Values\n",
    "\n",
    "You should also notice that the targets, sample_y, are the *next* value in the ordered test_text data. So, for an input sequence `[ 28,  29,  30,  31,  32]` that ends with the value `32`, the corresponding output should be `33`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[ 25,  26,  27,  28,  29],\n",
      "        [  4,   5,   6,   7,   8],\n",
      "        [ 38,  39,  40,  41,  42],\n",
      "        [ 11,  12,  13,  14,  15],\n",
      "        [ 15,  16,  17,  18,  19],\n",
      "        [ 37,  38,  39,  40,  41],\n",
      "        [ 39,  40,  41,  42,  43],\n",
      "        [  2,   3,   4,   5,   6],\n",
      "        [ 24,  25,  26,  27,  28],\n",
      "        [ 12,  13,  14,  15,  16]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([ 30,   9,  43,  16,  20,  42,  44,   7,  29,  17])\n"
     ]
    }
   ],
   "source": [
    "# test dataloader\n",
    "\n",
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Build the Neural Network\n",
    "Implement an RNN using PyTorch's [Module class](http://pytorch.org/docs/master/nn.html#torch.nn.Module). You may choose to use a GRU or an LSTM. To complete the RNN, you'll have to implement the following functions for the class:\n",
    " - `__init__` - The initialize function. \n",
    " - `init_hidden` - The initialization function for an LSTM/GRU hidden state\n",
    " - `forward` - Forward propagation function.\n",
    " \n",
    "The initialize function should create the layers of the neural network and save them to the class. The forward propagation function will use these layers to run forward propagation and generate an output and a hidden state.\n",
    "\n",
    "**The output of this model should be the *last* batch of word scores** after a complete sequence has been processed. That is, for each input sequence of words, we only want to output the word scores for a single, most likely, next word.\n",
    "\n",
    "### Hints\n",
    "\n",
    "1. Make sure to stack the outputs of the lstm to pass to your fully-connected layer, you can do this with `lstm_output = lstm_output.contiguous().view(-1, self.hidden_dim)`\n",
    "2. You can get the last batch of word scores by shaping the output of the final, fully-connected layer like so:\n",
    "\n",
    "```\n",
    "# reshape into (batch_size, seq_length, output_size)\n",
    "output = output.view(batch_size, -1, self.output_size)\n",
    "# get last batch\n",
    "out = output[:, -1]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        # TODO: Implement function\n",
    "        \n",
    "        # set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Add another LSTM, if possible\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden2output = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function  \n",
    "        batch_size = nn_input.size(0)\n",
    "        \n",
    "        embeds = self.embedding(nn_input)\n",
    "        \n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        output = self.dropout(lstm_out)\n",
    "        output = self.hidden2output(output)\n",
    "        \n",
    "        # reshape into (batch_size, seq_length, output_size)\n",
    "        output = output.view(batch_size, -1, self.output_size)\n",
    "        # get last batch\n",
    "        out = output[:, -1]\n",
    "\n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim).cuda(),\n",
    "                      torch.zeros(self.n_layers, batch_size, self.hidden_dim).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n",
    "                      torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define forward and backpropagation\n",
    "\n",
    "Use the RNN class you implemented to apply forward and back propagation. This function will be called, iteratively, in the training loop as follows:\n",
    "```\n",
    "loss = forward_back_prop(decoder, decoder_optimizer, criterion, inp, target)\n",
    "```\n",
    "\n",
    "And it should return the average loss over a batch and the hidden state returned by a call to `RNN(inp, hidden)`. Recall that you can get this loss by computing it, as usual, and calling `loss.item()`.\n",
    "\n",
    "**If a GPU is available, you should move your data to that GPU device, here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def detach(hidden):\n",
    "    return (hidden[0].detach(), hidden[1].detach())\n",
    "\n",
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "    \"\"\"\n",
    "    Forward and backward propagation on the neural network\n",
    "    :param decoder: The PyTorch Module that holds the neural network\n",
    "    :param decoder_optimizer: The PyTorch optimizer for the neural network\n",
    "    :param criterion: The PyTorch loss function\n",
    "    :param inp: A batch of input to the neural network\n",
    "    :param target: The target output for the batch of input\n",
    "    :return: The loss and the latest hidden state Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # move data to GPU, if available\n",
    "    if(train_on_gpu):\n",
    "         inp, target = inp.cuda(), target.cuda()\n",
    "\n",
    "    # perform backpropagation and optimization\n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    hidden = detach(hidden)\n",
    "    \n",
    "    output, h = rnn(inp, hidden)\n",
    "    \n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "\n",
    "    \n",
    "    # print(h.type())\n",
    "    # return the loss over a batch and the hidden state produced by our model\n",
    "    return loss.item(), h\n",
    "\n",
    "# Note that these tests aren't completely extensive.\n",
    "# they are here to act as general checks on the expected outputs of your functions\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "\n",
    "With the structure of the network complete and data ready to be fed in the neural network, it's time to train it.\n",
    "\n",
    "### Train Loop\n",
    "\n",
    "The training loop is implemented for you in the `train_decoder` function. This function will train the network over all the batches for the number of epochs given. The model progress will be shown every number of batches. This number is set with the `show_every_n_batches` parameter. You'll set this parameter along with other parameters in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    \n",
    "    loss_min = np.Inf\n",
    "    n_batches = len(train_loader.dataset)\n",
    "\n",
    "    for epoch_i in range(1, n_epochs + 1):\n",
    "        avg_losses = []\n",
    "        # initialize hidden state\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            # make sure you iterate over completely full batches, only\n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            # forward, back prop\n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            # record loss\n",
    "            batch_losses.append(loss)\n",
    "            avg_losses.append(loss)\n",
    "\n",
    "            # printing loss stats\n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}  Batch: {} / {} \\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses), batch_i, n_batches))\n",
    "                batch_losses = []\n",
    "        \n",
    "        avg_loss = np.average(avg_losses)\n",
    "        if avg_loss <= loss_min:\n",
    "            print(\"Loss decreased! {} -> {}. Saving the model...\".format(loss_min, avg_loss))\n",
    "            loss_min = avg_loss\n",
    "            torch.save(rnn.state_dict(), 'training_rnn_best.pt')\n",
    "        else:\n",
    "            print(\"Min loss so far: {}\").format(loss_min)\n",
    "\n",
    "    # returns a trained rnn\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Set and train the neural network with the following parameters:\n",
    "- Set `sequence_length` to the length of a sequence.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `num_epochs` to the number of epochs to train for.\n",
    "- Set `learning_rate` to the learning rate for an Adam optimizer.\n",
    "- Set `vocab_size` to the number of uniqe tokens in our vocabulary.\n",
    "- Set `output_size` to the desired size of the output.\n",
    "- Set `embedding_dim` to the embedding dimension; smaller than the vocab_size.\n",
    "- Set `hidden_dim` to the hidden dimension of your RNN.\n",
    "- Set `n_layers` to the number of layers/cells in your RNN.\n",
    "- Set `show_every_n_batches` to the number of batches at which the neural network should print progress.\n",
    "\n",
    "If the network isn't getting the desired results, tweak these parameters and/or the layers in the `RNN` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "# Sequence Length - analyze the median of sentence length from the text\n",
    "sequence_length = 7  # of words in a sequence\n",
    "# Batch Size\n",
    "batch_size = 100\n",
    "\n",
    "# data loader - do not change\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "# Number of Epochs\n",
    "num_epochs = 20\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Model parameters\n",
    "# Vocab size\n",
    "vocab_size = len(vocab_to_int)\n",
    "# Output size\n",
    "output_size = len(vocab_to_int)\n",
    "# Embedding Dimension\n",
    "embedding_dim = 400\n",
    "# Hidden Dimension\n",
    "hidden_dim = 1024\n",
    "# Number of RNN Layers\n",
    "n_layers = 1\n",
    "\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "In the next cell, you'll train the neural network on the pre-processed data.  If you have a hard time getting a good loss, you may consider changing your hyperparameters. In general, you may get better results with larger hidden and n_layer dimensions, but larger models take a longer time to train. \n",
    "> **You should aim for a loss less than 3.5.** \n",
    "\n",
    "You should also experiment with different sequence lengths, which determine the size of the long range dependencies that a model can learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epoch(s)...\n",
      "Epoch:    1/20    Loss: 5.368042888641358  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.821987474441529  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.718412603378296  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.594617066383361  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.555490509033203  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.495050009727478  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.48226537322998  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.42000597000122  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.407871356010437  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.385361808776856  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.406091053962707  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.412762092590332  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.37893666934967  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.36614647769928  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.324422768592834  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.316792817115783  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.4514195957183835  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.296703767776489  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.300436418533325  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.289691833496094  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.2990353384017945  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.2557446537017825  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.2531348400115965  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.23244932937622  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.23987789440155  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.236931422233582  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.255326803207398  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.2323990821838375  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.263430012702942  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.177966498374939  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.198194493293762  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.2357688274383545  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.225611792564392  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.20251904964447  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:    1/20    Loss: 4.15163712978363  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! inf -> 4.375452235599976. Saving the model...\n",
      "Epoch:    2/20    Loss: 4.060627228856936  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.928562418937683  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.9661026649475097  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.938876874923706  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.98131712436676  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.986537607192993  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.9570123109817503  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.994840461730957  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.9617483739852903  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.010363995552063  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.0071039323806765  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.00609331703186  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.9883012437820433  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.038768844604492  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.9789289436340334  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.024238535881042  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.9713548402786256  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.050692445755005  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.999538716316223  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.979598379135132  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.028242880821228  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.014853498458862  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.007962984085083  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.029098125457764  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.033181984901428  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.0175336093902585  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.024521091461182  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.039372444152832  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.004860702514648  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.004467181205749  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.03751600074768  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.9900918865203856  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 3.973168870925903  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.035584342002869  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:    2/20    Loss: 4.053147813796997  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 4.375452235599976 -> 4.0019818251876735. Saving the model...\n",
      "Epoch:    3/20    Loss: 3.855693104714509  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.7095233707427977  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.7580879764556885  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.7052578926086426  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.7423275623321532  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.7822920227050782  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.770754783630371  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.797898120880127  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.7985483474731447  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.807567645072937  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8407189960479737  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.803955888748169  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.7951948490142824  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.83522084236145  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.839733854293823  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8336272592544556  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8743665981292725  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8756113796234133  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8710434675216674  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8688237018585205  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8629023427963256  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8738925132751465  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8609174365997316  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.829476249694824  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.861786251068115  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.860223883628845  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8977044239044187  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.9251260509490966  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.9046326570510863  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.9024277715682985  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.8927458477020265  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.896447772026062  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.915374514579773  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.9310399732589723  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:    3/20    Loss: 3.9403346099853516  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 4.0019818251876735 -> 3.840960430278464. Saving the model...\n",
      "Epoch:    4/20    Loss: 3.735310034627303  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.6477075843811035  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.5893471660614016  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.6019872331619265  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.673742244720459  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.6384901514053345  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.650001106262207  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.6833012647628784  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.6598328905105593  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.705435311317444  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.692395770072937  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.7355035705566406  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.7255994853973387  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.739642233848572  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.73814547252655  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.69112021446228  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.7077970581054687  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.720874105453491  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.7386095390319825  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.753258267402649  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.7767793979644777  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.744771270751953  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.7558824396133423  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.770784441947937  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.763910786628723  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.773489731788635  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.7859876775741577  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.8257374210357664  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.786801671028137  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.815240881919861  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.8367411308288575  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.8260063581466675  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.8191880302429198  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.7996580362319947  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:    4/20    Loss: 3.848834635734558  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.840960430278464 -> 3.7341498650051714. Saving the model...\n",
      "Epoch:    5/20    Loss: 3.6302706657282813  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.53340891456604  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.5050634412765502  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.5045966148376464  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.531541712760925  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.578506527900696  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.58324556350708  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.558144424438477  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.6013139095306395  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.5859097776412963  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.607358200073242  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.64241571521759  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.6382243194580077  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.634318482398987  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.6477136478424073  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.7112054615020753  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.6449122200012205  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.627802138328552  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.6502427167892457  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.6630639514923096  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.6904970474243166  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.673076858520508  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.661286714553833  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.7149257249832153  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.7049342012405395  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.6857434396743773  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.770176766395569  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.7045929107666016  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.711575408935547  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.7185786962509155  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.731872438430786  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.721857305526733  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.7409780702590942  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.7284433784484863  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:    5/20    Loss: 3.751285186767578  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.7341498650051714 -> 3.648965248838693. Saving the model...\n",
      "Epoch:    6/20    Loss: 3.5501054290354674  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.464246241569519  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.43846192073822  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.4536480960845948  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.466374420166016  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.463451856613159  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.475009430885315  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.526702862739563  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.494615933418274  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.5160525321960447  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.541143105506897  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.5145183010101317  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.5559105615615847  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.5530535440444946  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.554543134689331  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.549674563407898  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.576200390815735  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.5892106943130493  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.610287205696106  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6177662477493286  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.613790054321289  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.590260199546814  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6028715705871583  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6539409408569337  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6695662355422973  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.644985058784485  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6419017238616944  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6536127586364744  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6705939331054687  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.654335410118103  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.665700547218323  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.693595782279968  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.663166606903076  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6808222103118897  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:    6/20    Loss: 3.6772652645111084  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.648965248838693 -> 3.5779812273964757. Saving the model...\n",
      "Epoch:    7/20    Loss: 3.484944951222798  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.3977801351547243  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.392855368614197  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.4031281423568727  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.4077805614471437  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.420411458015442  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.417654486656189  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.441232039451599  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.409894165992737  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.424427763938904  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.4417760200500487  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.467859354019165  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.4696280727386473  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.514697769165039  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.4948338012695315  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.5593851375579835  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.5586716060638426  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.504919404029846  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.522440166473389  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.5590072956085206  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.5562659397125245  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.555261769294739  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.5268875093460084  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.5949118814468384  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.5781101264953614  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.6003884201049803  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.5997583084106446  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.6445801458358766  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.634225254058838  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.618376905441284  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.592481556892395  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.6236111459732054  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.653988461494446  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.633135924339294  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:    7/20    Loss: 3.643472632408142  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.5779812273964757 -> 3.5232005405821565. Saving the model...\n",
      "Epoch:    8/20    Loss: 3.4296847137306194  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.2961007080078124  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.326690981864929  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.3546800365447997  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.3533217163085935  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.3437511825561526  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.3454361333847045  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.425484383583069  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.3838118772506713  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.4199496908187865  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.465937165260315  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.441997134208679  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.431810079574585  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.4432211360931397  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.4548303442001345  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.445936661720276  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.4998857679367066  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.507739772796631  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.4756071252822878  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.495932916641235  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.4911572999954226  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.511940281867981  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.531632779121399  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.5264623517990112  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.508119146347046  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.5680525436401367  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.5499275102615355  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.544186485290527  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.5973929767608643  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.5821231231689454  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.5703190755844116  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.564597725868225  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.569572692871094  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    8/20    Loss: 3.6200975170135496  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.5232005405821565 -> 3.474272232608348. Saving the model...\n",
      "Epoch:    9/20    Loss: 3.3956034710175067  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.2840549554824827  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.299649910926819  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.3251631898880003  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.281989212989807  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.335571711540222  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.32603289604187  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.3556338672637938  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.3651837644577025  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.3862570009231567  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.407593662261963  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.4033528661727903  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.392710031509399  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.40259686088562  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.3909483222961425  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.4285057039260862  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.3939579124450683  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.4652918367385865  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.4588648509979247  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.472325310707092  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.463163669586182  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.4907147340774536  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.503890655517578  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.5007537450790407  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.510692434310913  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.5063620901107786  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.54214004611969  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.533013997077942  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.5191995458602907  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.560717245101929  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.533165115356445  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.5311696214675905  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.569306803703308  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.5324677057266234  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:    9/20    Loss: 3.55710867023468  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.474272232608348 -> 3.4384277741372644. Saving the model...\n",
      "Epoch:   10/20    Loss: 3.3741677577591846  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.2391900329589842  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.241574887275696  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.2679330549240113  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.266664999961853  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.27931902217865  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.3011379470825197  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.3132033472061155  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.3129385185241698  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.3274766578674315  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.3261457471847535  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.3367681627273558  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.399028193473816  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.390518454551697  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4206360988616944  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4076444177627563  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.3858921518325804  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4086625547409057  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.436318468093872  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4184407081604005  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.442556262969971  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4378650369644164  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4479106435775755  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4721607284545897  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4688587474822996  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4852006521224976  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.492486788749695  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.5100414962768554  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.518478898048401  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4985806131362915  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.4876985263824465  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.528183439254761  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.5593560285568238  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.508022508621216  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   10/20    Loss: 3.5273469467163086  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.4384277741372644 -> 3.406280712567695. Saving the model...\n",
      "Epoch:   11/20    Loss: 3.3537323463557733  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.1736583518981933  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.199721405029297  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.238116905212402  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.2541350355148317  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.249187491416931  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.27219065952301  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.270214758872986  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.265298372268677  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.2944773387908937  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.3348602237701415  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.3080390853881836  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.315194414138794  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.3372799415588377  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.3579640102386477  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.390144386291504  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.3341068754196166  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4190316228866577  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.420616445541382  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.3957989625930787  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.432271322250366  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4144157848358154  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4266471576690676  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4284226331710816  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4581131343841554  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.438442406654358  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4725411729812623  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.5209863252639773  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4752472658157347  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.487167006492615  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4671264085769655  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.492344753265381  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.557359990119934  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.509376537322998  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   11/20    Loss: 3.4960712890625  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.406280712567695 -> 3.3778868002443088. Saving the model...\n",
      "Epoch:   12/20    Loss: 3.3474972253740543  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.2066697797775268  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.1614193725585937  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.210762890815735  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.2236668338775636  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.2380871019363404  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.2299133729934693  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.2636459436416625  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.2816165428161623  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.273796238899231  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.3012824087142945  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.3075213356018067  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.327134621620178  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.356011407852173  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.3275434284210204  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.3360128679275514  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.337857014656067  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.366368335723877  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.335223098754883  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.388802985191345  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.3750398054122925  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.408579560279846  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.4673986682891846  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.4351630001068116  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.4014527282714844  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.3729459772109984  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.4412277450561524  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.43472492313385  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.4627451515197754  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.4859008026123046  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.4655132684707644  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.4742786321640016  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.481314582824707  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.485432858467102  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   12/20    Loss: 3.488755464553833  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.3778868002443088 -> 3.3562910068250265. Saving the model...\n",
      "Epoch:   13/20    Loss: 3.2979736095935883  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.1330202779769896  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.1988184938430786  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.1924623441696167  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.212000982284546  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.2186397466659544  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.2350367155075075  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.2495832901000976  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.293096297264099  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.2605530214309693  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.2414817514419556  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.298060980796814  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.296956642150879  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.249124158859253  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.3355415592193602  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.3049829387664795  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.354487347602844  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.3622090320587157  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.366091481208801  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.364123330116272  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.3774767274856567  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4076604356765747  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.422284372329712  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4468653774261475  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4206107501983642  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.374379033088684  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4077275133132936  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4353674154281615  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4098195915222167  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4523445987701415  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4275348443984983  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4382577266693115  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4451575021743777  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4658381767272948  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   13/20    Loss: 3.4907450008392336  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.3562910068250265 -> 3.3385681661037756. Saving the model...\n",
      "Epoch:   14/20    Loss: 3.270571718872868  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.131745572090149  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.1506327257156372  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.184512745857239  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.1874386835098267  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.239209515571594  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.1962387962341308  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.207398516654968  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.2242644243240357  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.244210333824158  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.238071912765503  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.2819507188797  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.275328890800476  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.3170111713409423  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.2835644245147706  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.304659482002258  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.343723054885864  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.3741982374191286  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.3511943826675417  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.378291368484497  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.3684856843948365  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.362615330696106  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.3672662181854247  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.379014039993286  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.3763318166732788  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.3815710010528566  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.4276935682296754  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.432530511856079  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.3990155553817747  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.4420170345306396  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.4349522495269778  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.4602470388412474  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.4361520919799804  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.4527096939086914  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   14/20    Loss: 3.468575954437256  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.3385681661037756 -> 3.322768492651098. Saving the model...\n",
      "Epoch:   15/20    Loss: 3.2407495035411626  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.1057233123779295  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.1262001304626463  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.2139159088134766  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.15636554813385  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.193142930984497  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.1913406581878663  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.185631881713867  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.239144330024719  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.259752124786377  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.2500203924179076  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.2548032236099242  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.2744704456329345  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.2586692543029785  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.2745539340972902  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.2915079708099366  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.3037751445770263  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.356596724510193  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.314106439590454  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.3482174224853516  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.3406475276947023  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.3674136524200438  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.3082206001281738  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.377239315986633  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.3976567964553834  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.38939862537384  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.397088324546814  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.363682427406311  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.4029086332321166  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.4090523777008057  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.410506283760071  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.446612138748169  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.478294424057007  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.453882481575012  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   15/20    Loss: 3.463985733985901  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.322768492651098 -> 3.3086574538354583. Saving the model...\n",
      "Epoch:   16/20    Loss: 3.2474637841385503  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.1187395849227904  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.131076259613037  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.1458583822250366  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.160981441497803  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.1959365711212158  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.1839169664382934  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.2085626039505004  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.212117117881775  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.207333172798157  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.256848895072937  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.235101315498352  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.213298698425293  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.271705674171448  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.2706918172836303  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.289682158470154  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.3011566715240477  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.320276396751404  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.336809992790222  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.3277380084991455  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.329187021255493  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.3674149036407472  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.346265838623047  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.360010006904602  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.3858822536468507  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.377835594177246  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.4017418384552003  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.3732921981811526  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.3926902856826784  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.3579357461929322  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.4162165355682372  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.425653916358948  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.426622462272644  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.4429711580276487  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   16/20    Loss: 3.4674435577392577  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.3086574538354583 -> 3.299625171458904. Saving the model...\n",
      "Epoch:   17/20    Loss: 3.24836775752541  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.11803990650177  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.1419890842437743  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.184109871864319  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.148680824279785  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.156608295440674  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.1828343448638914  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.207066287994385  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.2118356008529663  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.207586576461792  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.249469331741333  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.2687050533294677  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.255439043045044  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.2726978578567505  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.2446915073394775  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.258377427101135  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.311358967781067  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.2929495792388916  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.286106225967407  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.3043144817352297  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.3051087799072265  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.324207240104675  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.367062812805176  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.352020574569702  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.3538541049957273  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.387367136001587  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.366909342765808  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.36410142993927  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.390032914161682  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.388728268623352  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.400461537361145  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.4352334032058716  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.3840757789611815  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.4228623456954956  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   17/20    Loss: 3.447319815635681  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.299625171458904 -> 3.2906977737742333. Saving the model...\n",
      "Epoch:   18/20    Loss: 3.22727287183748  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.1066261672973634  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.0925944948196413  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.1396819620132446  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.158228567123413  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.140254963874817  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.1896532106399538  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.167011416435242  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.2250480289459227  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.210007911682129  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.245388828277588  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.1920817337036134  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.2390009937286375  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.2110821981430053  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.2706982078552245  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.265635350227356  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.285155858039856  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.272931489944458  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.2864883670806884  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.316534014701843  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.2972346954345704  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.286271848678589  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.35369019985199  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.346744490623474  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.4028713932037356  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.3699959259033205  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.3849588642120363  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.3958734731674194  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.396806873321533  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.402688793182373  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.400044327735901  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.4269182586669924  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.3947362546920776  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.4081790685653686  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   18/20    Loss: 3.430408440589905  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.2906977737742333 -> 3.2828296091160745. Saving the model...\n",
      "Epoch:   19/20    Loss: 3.2278524170012486  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.1031777639389038  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.1072762250900268  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.136520339012146  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.127986692428589  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.1862595081329346  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.1365572595596314  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.1807652492523193  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.2328209028244017  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.2005237579345702  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.1970700998306274  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.211790910720825  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.230931535720825  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.2518825492858885  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.2627551231384277  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.2427655906677244  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.292062762260437  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.2903681659698485  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3179250717163087  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3021808986663816  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.2959100294113157  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.335991340637207  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3480723676681516  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3353040285110476  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.375748579978943  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.360593994140625  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3487434949874877  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3515211486816407  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3744202575683593  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3649788265228273  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.3850168867111208  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.381606598854065  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.344615837097168  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.402317400932312  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   19/20    Loss: 3.379735054016113  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.2828296091160745 -> 3.275073364435484. Saving the model...\n",
      "Epoch:   20/20    Loss: 3.2378305159951615  Batch: 250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.0800022392272948  Batch: 500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.140148415565491  Batch: 750 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.1130916566848756  Batch: 1000 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.1208611927032472  Batch: 1250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.1180968341827393  Batch: 1500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.160726998329163  Batch: 1750 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.1797500772476197  Batch: 2000 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.1848742694854737  Batch: 2250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.2299943504333495  Batch: 2500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.226748517036438  Batch: 2750 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.2019884395599365  Batch: 3000 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.2321139879226686  Batch: 3250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.2703117151260375  Batch: 3500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.272751646995544  Batch: 3750 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.2552341241836547  Batch: 4000 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.267966938018799  Batch: 4250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.2810580320358276  Batch: 4500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3149050226211547  Batch: 4750 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.2685880184173586  Batch: 5000 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3262690258026124  Batch: 5250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3290424156188965  Batch: 5500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3172743797302244  Batch: 5750 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3262018308639525  Batch: 6000 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3343583059310915  Batch: 6250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.319947603225708  Batch: 6500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3575910062789918  Batch: 6750 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.358516046524048  Batch: 7000 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3824828596115113  Batch: 7250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.38172744178772  Batch: 7500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3458622493743895  Batch: 7750 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.382787760734558  Batch: 8000 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.4094886350631715  Batch: 8250 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3587009382247923  Batch: 8500 / 8921 \n",
      "\n",
      "Epoch:   20/20    Loss: 3.3986105337142942  Batch: 8750 / 8921 \n",
      "\n",
      "Loss decreased! 3.275073364435484 -> 3.2690851075568284. Saving the model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "# create model and move to gpu if available\n",
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "# defining loss and optimization functions for training\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training the model\n",
    "with active_session():\n",
    "    trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "# saving the trained model\n",
    "helper.save_model('./save/trained_rnn_5', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How did you decide on your model hyperparameters? \n",
    "For example, did you try different sequence_lengths and find that one size made the model converge faster? What about your hidden_dim and n_layers; how did you decide on those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:** \n",
    "\n",
    "1. Sequence_length - I tried with different sequence lengths. Initially, I started with size 12, since I was looking at the sentences in text data, but apparently, it didn't work out quite well. I tried with 10 and later I decided with 7 and it worked well, the generated script was pretty good, I was impressed.\n",
    "2. Batch_size - This one was picked by intuition, honestly. I didn't know, I tried with 32, then with 64 and then increased it to 100, which made some progress. I haven't tried anything else.\n",
    "3. Initially, 15 epochs, but I think it hasn't converged yet, so I did with 20 epochs and I still think it coul have trained more.\n",
    "4. I've set 0.001, \"standard\" learning rate.\n",
    "5. Vocab_size - pretty straight forward, just picked the length of vocabulary size\n",
    "6. Output_size - this one, my thinking was if I need the generated text, I need some kind of probability list of all words what words has the highest probability, so I decided it is also the lenght of the vocabulary.\n",
    "7. Embedding_dim - since I have added embedding, I have decided to put 400. I tried with other dimensions, but this one has worked out for me (300, 450, 600).\n",
    "8. Hidden_dim - Honestly, I also picked this intuitively, I have picked in the end 1024. I tried with 512, but it hasn't worked out, so I have increased and it worked. I think that it was enough to contain so much data, since we were processing some \"every day\" talk, so to speak.\n",
    "9. n_layers - The same reasoning as I had with hidden_dim, initially, I tried with 2 and tried with 1, the 1 layer proved to be better in this case. I guess it was more flexible to train and also, we weren't processing some scientific texts and translations, we were trying to generate words and I think one layer is more than enough to do the job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Checkpoint\n",
    "\n",
    "After running the above training cell, your model will be saved by name, `trained_rnn`, and if you save your notebook progress, **you can pause here and come back to this code at another time**. You can resume your progress by running the next cell, which will load in our word:id dictionaries _and_ load in your saved model by name!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script\n",
    "With the network trained and saved, you'll use it to generate a new, \"fake\" Seinfeld TV script in this section.\n",
    "\n",
    "### Generate Text\n",
    "To generate the text, the network needs to start with a single word and repeat its predictions until it reaches a set length. You'll be using the `generate` function to do this. It takes a word id to start with, `prime_id`, and generates a set length of text, `predict_len`. Also note that it uses topk sampling to introduce some randomness in choosing the most likely next word, given an output set of word scores!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    \"\"\"\n",
    "    Generate text using the neural network\n",
    "    :param decoder: The PyTorch Module that holds the trained neural network\n",
    "    :param prime_id: The word id to start the first prediction\n",
    "    :param int_to_vocab: Dict of word id keys to word values\n",
    "    :param token_dict: Dict of puncuation tokens keys to puncuation values\n",
    "    :param pad_value: The value used to pad a sequence\n",
    "    :param predict_len: The length of text to generate\n",
    "    :return: The generated text\n",
    "    \"\"\"\n",
    "    rnn.eval()\n",
    "    \n",
    "    # create a sequence (batch_size=1) with the prime_id\n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        # get the output of the rnn\n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        # get the next word probabilities\n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "         \n",
    "        # use top_k sampling to get the index of the next word\n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next word index with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        # retrieve that word from the dictionary\n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        # the generated word becomes the next \"current sequence\" and the cycle can continue\n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    # Replace punctuation tokens\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    # return all the sentences\n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a New Script\n",
    "It's time to generate the text. Set `gen_length` to the length of TV script you want to generate and set `prime_word` to one of the following to start the prediction:\n",
    "- \"jerry\"\n",
    "- \"elaine\"\n",
    "- \"george\"\n",
    "- \"kramer\"\n",
    "\n",
    "You can set the prime word to _any word_ in our dictionary, but it's best to start with a name for generating a TV script. (You can also start with any other names you find in the original text file!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:43: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry:\n",
      "\n",
      "kramer: well, i gotta go to the beach on.\n",
      "\n",
      "jerry: i don't have to get a massage from me, you know, i was a little nervous...\n",
      "\n",
      "jerry: what?\n",
      "\n",
      "elaine:(smiling) yeah! well, what do you do?\n",
      "\n",
      "jerry: no.\n",
      "\n",
      "kramer:(singing) master of the apartment.\n",
      "\n",
      "elaine:(looking at her watch) oh... i mean, what are we gonna do for the show?\n",
      "\n",
      "elaine: i don't know, maybe.\n",
      "\n",
      "elaine:(to kramer) you know, i don't want to talk to him.\n",
      "\n",
      "elaine:(to george) so what happened?\n",
      "\n",
      "jerry: no, i don't think so.\n",
      "\n",
      "george: what?\n",
      "\n",
      "jerry: well, you know, the whole thing is, she had a good time.\n",
      "\n",
      "jerry:(looking at his shoes) you have to go for the big salad?\n",
      "\n",
      "george: yeah, that's the way i have.\n",
      "\n",
      "kramer:(to jerry) hey, i just saw him on the street. you know what?\n",
      "\n",
      "george: you think i should be going to be able to get out of your house.\n",
      "\n",
      "jerry:(to kramer) hey! hey.\n",
      "\n",
      "george:(still on a phone) oh, i don't know what to be...........\n",
      "\n",
      "elaine: what?\n",
      "\n",
      "jerry: you have the money! i don't know how much i get it?\n",
      "\n",
      "newman:(to jerry) what do you think, jerry?\n",
      "\n",
      "jerry: i don't know, what are you doing?\n",
      "\n",
      "kramer: oh yeah, i know.\n",
      "\n",
      "elaine: what about the guy from the bathroom.\n",
      "\n",
      "elaine: well, what am i gonna do?\n",
      "\n",
      "george: i don't know, you know, i was wondering if we have the job.\n",
      "\n",
      "elaine: i know. i was just going to have to be a little more.\n",
      "\n",
      "jerry: so, what\n"
     ]
    }
   ],
   "source": [
    "# run the cell multiple times to get different results!\n",
    "gen_length = 400 # modify the length to your preference\n",
    "prime_word = 'jerry' # name for starting the script\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save your favorite scripts\n",
    "\n",
    "Once you have a script that you like (or find interesting), save it to a text file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save script to a text file\n",
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TV Script is Not Perfect\n",
    "It's ok if the TV script doesn't make perfect sense. It should look like alternating lines of dialogue, here is one such example of a few generated lines.\n",
    "\n",
    "### Example generated script\n",
    "\n",
    ">jerry: what about me?\n",
    ">\n",
    ">jerry: i don't have to wait.\n",
    ">\n",
    ">kramer:(to the sales table)\n",
    ">\n",
    ">elaine:(to jerry) hey, look at this, i'm a good doctor.\n",
    ">\n",
    ">newman:(to elaine) you think i have no idea of this...\n",
    ">\n",
    ">elaine: oh, you better take the phone, and he was a little nervous.\n",
    ">\n",
    ">kramer:(to the phone) hey, hey, jerry, i don't want to be a little bit.(to kramer and jerry) you can't.\n",
    ">\n",
    ">jerry: oh, yeah. i don't even know, i know.\n",
    ">\n",
    ">jerry:(to the phone) oh, i know.\n",
    ">\n",
    ">kramer:(laughing) you know...(to jerry) you don't know.\n",
    "\n",
    "You can see that there are multiple characters that say (somewhat) complete sentences, but it doesn't have to be perfect! It takes quite a while to get good results, and often, you'll have to use a smaller vocabulary (and discard uncommon words), or get more data.  The Seinfeld dataset is about 3.4 MB, which is big enough for our purposes; for script generation you'll want more than 1 MB of text, generally. \n",
    "\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save another copy as an HTML file by clicking \"File\" -> \"Download as..\"->\"html\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission. Once you download these files, compress them into one zip file for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
